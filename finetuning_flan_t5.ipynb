{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Low Rank Adaptation and Parameter Efficient Finetuning of HuggingFace Flan-T5 LLMs on Text Summarisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, torch, json, random, gc\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"LLM_REPOSITORY\"] = \"google/flan-t5-small\"\n",
    "os.environ[\"TOKENIZER_REPOSITORY\"] = \"google/flan-t5-small\"\n",
    "os.environ[\"EMBEDDINGS_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "os.environ[\"MAX_TOKENS\"] = \"4096\"\n",
    "os.environ[\"DEVICE\"] = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"DATASET_PATH\"] = \"data/doc_summary_data\"\n",
    "os.environ[\"TOKENS_DATA_PATH\"] = F\"data/doc_summary_{os.environ['TOKENIZER_REPOSITORY'].split('/')[-1]}_tokens\"\n",
    "os.environ[\"SUMMARY_DATA_PATH\"] = \"data/doc_summary_pair.json\"\n",
    "os.makedirs(os.environ[\"DATASET_PATH\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TOKENS_DATA_PATH\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset into Training, Validation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VALIDATION_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "with open(os.environ[\"SUMMARY_DATA_PATH\"]) as f:\n",
    "    doc_summary_data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "random.shuffle(doc_summary_data)\n",
    "train_size = int(len(doc_summary_data) * TRAIN_SIZE)\n",
    "val_size = int(len(doc_summary_data) * VALIDATION_SIZE)\n",
    "test_size = int(len(doc_summary_data) * TEST_SIZE)\n",
    "\n",
    "train_data = doc_summary_data[:train_size]\n",
    "val_data = doc_summary_data[train_size:train_size+val_size]\n",
    "test_data = doc_summary_data[train_size+val_size:]\n",
    "\n",
    "data_list = [\n",
    "    (\"train\", train_data),\n",
    "    (\"validation\", val_data),\n",
    "    (\"test\", test_data),\n",
    "]\n",
    "\n",
    "for data_tuple in data_list:\n",
    "    with open(os.path.join(os.environ[\"DATASET_PATH\"], f\"{data_tuple[0]}.json\"), \"w\") as f:\n",
    "        json.dump(data_tuple[1], f, indent=4)\n",
    "    f.close()\n",
    "\n",
    "del doc_summary_data, train_data, val_data, test_data, data_list, data_tuple, train_size, val_size, test_size\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset into DictDataset Format to be modelled by the HuggingFace LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99017765041c4aaca505ddbbdb6cfd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab7674c6d5e4bfaa89f4ffa1aa508a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e7c85c9184441aa490b19f6d1e4c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac6b7f816bf4aad831f72095eddae97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddd3adc984949438f93813aa5b5ca13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4000\n",
      "Validation dataset size: 500\n",
      "Test dataset size: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'document': 'NCT Number: NCT03317405\\nStudy Title: Phase I Trial of Endoxifen Gel Versus Placebo in Women Undergoing Breast Surgery\\nStudy URL: https://beta.clinicaltrials.gov/study/NCT03317405\\nAcronym: unknown\\nStudy Status: ACTIVE_NOT_RECRUITING\\nBrief Summary: This is a randomized, double-blind, Phase I trial with dose escalation from 10 mg endoxifen (ENX) gel daily (5mg per breast) to 20 mg daily (10mg per breast) in women undergoing mastectomy. Endoxifen hydrochloride may treat or reduce the risk of breast cancer.\\nStudy Results: YES\\nConditions: Breast Ductal Carcinoma In Situ|Breast Lobular Carcinoma In Situ|Stage 0 Breast Cancer AJCC v6 and v7|Stage I Breast Cancer AJCC v7|Stage IA Breast Cancer AJCC v7|Stage IB Breast Cancer AJCC v7|Stage II Breast Cancer AJCC v6 and v7|Stage IIA Breast Cancer AJCC v6 and v7|Stage IIB Breast Cancer AJCC v6 and v7|Stage III Breast Cancer AJCC v7|Stage IIIA Breast Cancer AJCC v7|Stage IIIB Breast Cancer AJCC v7|Stage IIIC Breast Cancer AJCC v7',\n",
       " 'summary': 'A Phase I trial is testing the effectiveness of endoxifen gel in women undergoing breast surgery to determine if it can treat or reduce the risk of breast cancer. The study is currently active but not recruiting participants.',\n",
       " 'id': 1600}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(path=os.environ[\"DATASET_PATH\"])\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset['validation'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corresponding LLM Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661dcbaa1cce4ec1b3c388fd1c58b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7123f7dc054f444cad0c22df4512bca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923af2f4618f4647b564eb30900d99ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b106aaeb209a46b38001bb906a592aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    os.environ[\"TOKENIZER_REPOSITORY\"],\n",
    "    model_max_length=int(os.environ[\"MAX_TOKENS\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tokenizer Object to retreive the Maximum Source (Text) and Target (Summary) Tokens in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16dcfbcb0eb4a24ba3d8c08ebb3eeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042b2f7b517247659a332bf5c9702987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 428\n",
      "Max target length: 220\n"
     ]
    }
   ],
   "source": [
    "concatenated_dataset = concatenate_datasets(\n",
    "    [dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]\n",
    ")\n",
    "tokenized_inputs = concatenated_dataset.map(\n",
    "    lambda x: tokenizer(x[\"document\"], truncation=True), batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "\n",
    "tokenized_targets = concatenated_dataset.map(\n",
    "    lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset and Persist Tokens to Disk Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84426b2fa7d4175b4af36c53b11b392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbae187c9760411491cdb8a6a6ce3407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49385dc83fb497ba2111b98e88ac708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecb369fba2c42388a5954399021a711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1d09a0d0514c6eaa8241c3e74dd6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4889632f2c47929c2a953a57c8dfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample, max_source_length: int, max_target_length: int, padding: str=\"max_length\"):\n",
    "    inputs = [f\"summarize this document: {item}\"  for item in sample[\"document\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_source_length, \n",
    "        padding=padding, \n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=sample[\"summary\"], \n",
    "        max_length=max_target_length,\n",
    "        padding=padding, \n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # replace pad tokens with -100, this is because pad tokens hold no semantic meaning, and thus should not \n",
    "    # contribute to the loss value, -100 is choosen because it is unlikely be be an ID to any given token in \n",
    "    # tokenizer\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "preprocess_lambda = lambda dataset : preprocess_function(dataset, max_source_length, max_target_length)\n",
    "tokenized_dataset = dataset.map(preprocess_lambda, batched=True, remove_columns=[\"document\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"train\"))\n",
    "tokenized_dataset[\"validation\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"validation\"))\n",
    "tokenized_dataset[\"test\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 8bits quantized HuggingFace LLM to Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5d26569e834403a5d1a32d6fb05911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7ebcf92d4841dc80a918e68f4fabe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d1d20e83f644d79755de57e4c7db4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.environ[\"LLM_REPOSITORY\"],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Low Rank Adaptation Configurations Object and apply to Loaded LLM for Parameter Efficient Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 77,649,280 || trainable%: 0.8862001038515747\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r=16, \n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Collator Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL FINETUNING / TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Seq2SeqTrainer Object and Commence LoRA Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 2:09:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.365900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.058300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=1.2296947509765626, metrics={'train_runtime': 7760.7051, 'train_samples_per_second': 10.308, 'train_steps_per_second': 1.289, 'total_flos': 1.26902992896e+16, 'train_loss': 1.2296947509765626, 'epoch': 20.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR = F\"lora-{os.environ['LLM_REPOSITORY'].split('/')[-1]}\"\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\tauto_find_batch_size=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # to be set to True for inference\n",
    "\n",
    "# finetune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist LoRA Model Weights to Disk Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flan-t5-small_finetuned_results/tokenizer_config.json',\n",
       " 'flan-t5-small_finetuned_results/special_tokens_map.json',\n",
       " 'flan-t5-small_finetuned_results/spiece.model',\n",
       " 'flan-t5-small_finetuned_results/added_tokens.json',\n",
       " 'flan-t5-small_finetuned_results/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "PEFT_MODEL_ID=f\"{os.environ['LLM_REPOSITORY'].split('/')[-1]}_finetuned_results\"\n",
    "trainer.model.save_pretrained(PEFT_MODEL_ID)\n",
    "tokenizer.save_pretrained(PEFT_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LoRA Weights from Disk to Perform Inference on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peft config for pre-trained checkpoint etc. \n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL_ID)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL_ID, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Loaded Model and Tokenizer to Instantiate a Langchain HuggingFacePipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSeq2SeqLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "# switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# define model pipeline\n",
    "hgf_pipeline = pipeline(\n",
    "    task=\"text2text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.1, \n",
    "    max_length=int(os.environ[\"MAX_TOKENS\"]),\n",
    "    top_p=0.15,\n",
    "    top_k=0,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hgf_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Summaries from Documents in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c458f22aaa4914ae6e018654b67021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8314451e7d24499ea197f6f52cdf2525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2268aea64094be1b42498c886355c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484c0252f67042fcafc8ad1009aa8223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: page_content='NCT Number: NCT03049449\\nStudy Title: T Cells Expressing a Fully-Human Anti-CD30 Chimeric Antigen Receptor for Treating CD30-Expressing Lymphomas\\nStudy URL: https://beta.clinicaltrials.gov/study/NCT03049449\\nAcronym: unknown\\nStudy Status: COMPLETED\\nBrief Summary: Background:' metadata={}\n",
      "\n",
      "SUMARY: A study has been conducted to determine the effectiveness of a fully-human anti-CD30 chimeric antigen receptor for treating CD30-expressing lymphomas.\n",
      "\n",
      "\n",
      "Document: page_content='Interventions: DRUG: Ibrutinib|OTHER: Laboratory Biomarker Analysis\\nPrimary Outcome Measures: Incidence of relapsed leukemia, Incidence of relapsed leukemia defined as \\\\> 5% leukemic blasts detected in bone marrow or peripheral blood. Participants will also be considered to have relapsed leukemia if they receive any active treatment for progressive leukemia after allogeneic HCT, even if they have \\\\< 5% leukemic blasts. Withdrawal of immunosuppression alone is not considered an active treatment for progressive disease., Up to 18 months\\nSecondary Outcome Measures: Incidence of acute GVHD grades II-IV and III-IV, Incidence of acute GVHD grades II-IV and III-IV will be evaluated according following criteria.' metadata={}\n",
      "\n",
      "SUMARY: This study will evaluate the effectiveness of Ibrutinib and laboratory biomarker analysis in treating relapsed leukemia, including patients with acute GVHD grades II-IV and III-IV and the secondary outcome.\n",
      "\n",
      "\n",
      "Document: page_content='54476, United States|Aspirus UW Cancer Center, Wisconsin Rapids, Wisconsin, 54494, United States|Marshfield Clinic - Wisconsin Rapids Center, Wisconsin Rapids, Wisconsin, 54494, United States|Cheyenne Regional Medical Center-West, Cheyenne, Wyoming, 82001, United States|Billings Clinic-Cody, Cody, Wyoming, 82414, United States|Welch Cancer Center, Sheridan, Wyoming, 82801, United States' metadata={}\n",
      "\n",
      "SUMARY: The summary provides a list of cancer centers and clinics in Wisconsin and Wyoming, including their addresses.\n",
      "\n",
      "\n",
      "Document: page_content=\"Cancer Center at Cooper-Voorhees, Voorhees, New Jersey, 08043, United States|University of New Mexico Cancer Center, Albuquerque, New Mexico, 87102, United States|Southwest Gynecologic Oncology Associates Inc, Albuquerque, New Mexico, 87106, United States|Lovelace Radiation Oncology, Albuquerque, New Mexico, 87109, United States|Memorial Medical Center - Las Cruces, Las Cruces, New Mexico, 88011, United States|Women's Cancer Care Associates LLC, Albany, New York, 12208, United States|Montefiore Medical Center-Einstein Campus, Bronx, New York, 10461, United States|State University of New York Downstate Medical Center, Brooklyn, New York, 11203, United States|New York-Presbyterian/Brooklyn Methodist Hospital, Brooklyn, New York, 11215, United States|Roswell Park Cancer Institute, Buffalo, New York, 14263, United States|Memorial Sloan Kettering Commack, Commack, New York, 11725, United States|New York Hospital Medical Center of Queens, Fresh Meadows, New York, 11365, United States|Memorial Sloan Kettering\" metadata={}\n",
      "\n",
      "SUMARY: The summary provides a list of cancer centers and hospitals in New Jersey and New York, including their addresses.\n",
      "\n",
      "\n",
      "Document: page_content='Carfilzomib is FDA (Food and Drug Administration) approved in the United States for adults with multiple myeloma (a type of cancer). However, this drug is not approved to treat children with relapsed/refractory solid tumors or leukemia. With this research, we plan to determine the DLTs and MTD of Carfilzomib given in combination with cyclophosphamide and etoposide in pediatric patients with relapsed/refractory leukemias and solid tumors.\\nStudy Results: NO\\nConditions: Relapsed Solid Tumors|Refractory Solid Tumors|Relapsed Leukemia|Refractory Leukemia\\nInterventions: DRUG: Carfilzomib|DRUG: Cyclophosphamide|DRUG: Etoposide\\nPrimary Outcome Measures: To determine the DLTs and MTD of carfilzomib given in combination with cyclophosphamide and etoposide in pediatric patients with relapsed/refractory leukemias and solid tumors, 30 Days post treatment initiation' metadata={}\n",
      "\n",
      "SUMARY: This study is investigating the effectiveness of carfilzomib in treating multiple myeloma. It aims to determine the DLTs and MTD of carfilzomib in combination with cyclophosphamide and etoposide in pediatric patients with relapsed/refractory leukemias and solid tumors. The primary outcome measure is to determine the DLTs and MTD of carfilzomib in combination with cyclophosphamide and etoposide.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "# summarise first 5 documents in the testing data\n",
    "predicted_summaries = []\n",
    "n_docs = 5\n",
    "for i, document in enumerate(dataset[\"test\"][\"document\"][:n_docs]):\n",
    "    document = Document(page_content=document)\n",
    "    summary = summary_chain.run([document])\n",
    "    print(f\"Document: {document}\\n\")\n",
    "    print(f\"SUMARY: {summary}\\n\\n\")\n",
    "    predicted_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PERFORMANCE MEASUREMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Generated Summaries to Target Summaries with the Rouge Score and the Cosine Similarity Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfb858cb2a5438f96587e3b1ab9d920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5dded/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0284321b6fce4dc4a769adec7358800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8864a69ec21e476a84e689e0424444d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)4d81d5dded/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4c69f1f8cd48a89b629b6ba38733f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)81d5dded/config.json:   0%|          | 0.00/573 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadeb6e5dd3f4ea6bca09d0cfd20ab5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90d10e68c174776a17e03907b6a25d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ded/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f2765051b8424fa826a8f0c3590760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd597b3debc4692911b6ddc23898a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d718a72c80a441b08e0b06210478c592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8f9252cce344128c72a56739bac98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5dded/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41f15a1f2774862bec833ed10a3c72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc73d72181d4387ad077eb271617dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)dded/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dfbbdbaec7460ea40f43e6baadd4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)4d81d5dded/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdede27bd204a3880a481c5440504cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1d5dded/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for summary 1: 0.6672851 \n",
      "\n",
      "Rouge scores for summary 1: {'rouge-1': {'r': 0.4642857142857143, 'p': 0.65, 'f': 0.5416666618055557}, 'rouge-2': {'r': 0.21428571428571427, 'p': 0.3157894736842105, 'f': 0.2553191441195111}, 'rouge-l': {'r': 0.4642857142857143, 'p': 0.65, 'f': 0.5416666618055557}} \n",
      "\n",
      "\n",
      "Cosine similarity for summary 2: 0.735101 \n",
      "\n",
      "Rouge scores for summary 2: {'rouge-1': {'r': 0.3870967741935484, 'p': 0.46153846153846156, 'f': 0.42105262661742077}, 'rouge-2': {'r': 0.14285714285714285, 'p': 0.17857142857142858, 'f': 0.1587301537918873}, 'rouge-l': {'r': 0.3225806451612903, 'p': 0.38461538461538464, 'f': 0.3508771880209296}} \n",
      "\n",
      "\n",
      "Cosine similarity for summary 3: 0.7573644 \n",
      "\n",
      "Rouge scores for summary 3: {'rouge-1': {'r': 0.4444444444444444, 'p': 0.75, 'f': 0.5581395302109249}, 'rouge-2': {'r': 0.1875, 'p': 0.375, 'f': 0.2499999955555556}, 'rouge-l': {'r': 0.4074074074074074, 'p': 0.6875, 'f': 0.5116279023039481}} \n",
      "\n",
      "\n",
      "Cosine similarity for summary 4: 0.8874081 \n",
      "\n",
      "Rouge scores for summary 4: {'rouge-1': {'r': 0.5238095238095238, 'p': 0.6470588235294118, 'f': 0.5789473634764544}, 'rouge-2': {'r': 0.391304347826087, 'p': 0.5, 'f': 0.439024385318263}, 'rouge-l': {'r': 0.5238095238095238, 'p': 0.6470588235294118, 'f': 0.5789473634764544}} \n",
      "\n",
      "\n",
      "Cosine similarity for summary 5: 0.8400016 \n",
      "\n",
      "Rouge scores for summary 5: {'rouge-1': {'r': 0.5128205128205128, 'p': 0.6060606060606061, 'f': 0.555555550590278}, 'rouge-2': {'r': 0.36538461538461536, 'p': 0.475, 'f': 0.4130434733459358}, 'rouge-l': {'r': 0.5128205128205128, 'p': 0.6060606060606061, 'f': 0.555555550590278}} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "embeddings_model = SentenceTransformer(os.environ[\"EMBEDDINGS_MODEL\"])\n",
    "embeddings_model.to(os.environ[\"DEVICE\"])\n",
    "target_summaries = dataset[\"test\"][\"summary\"][:n_docs]\n",
    "\n",
    "for i, (predicted_summary, target_summary) in enumerate(zip(predicted_summaries, target_summaries)):\n",
    "    pred_embeddings, target_embeddings = (\n",
    "        embeddings_model.encode(predicted_summary).reshape(1, -1),\n",
    "        embeddings_model.encode(target_summary).reshape(1, -1)\n",
    "    )\n",
    "    cos_similarity = cosine_similarity(target_embeddings, pred_embeddings)\n",
    "    rouge_scores = rouge.get_scores(predicted_summary, target_summary)\n",
    "    print(f\"Cosine similarity for summary {i+1}:\", cos_similarity[0][0], \"\\n\")\n",
    "    print(f\"Rouge scores for summary {i+1}:\", rouge_scores[0], \"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
